{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV de Chile descargado exitosamente.\n",
      "Archivo CSV de Japón descargado exitosamente.\n",
      "Archivo CSV de Estados Unidos descargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "# Juntamos los requests para obtener los datos de los tres paises que nos interesan a traves de la API\n",
    "\n",
    "chile_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "chile_params = {\n",
    "    \"format\": \"csv\",\n",
    "    \"starttime\": \"2012-01-01 00:00:00\",\n",
    "    \"endtime\": \"2023-06-26 23:59:59\",\n",
    "    \"minlatitude\": -56.8,\n",
    "    \"maxlatitude\": -19.0,\n",
    "    \"minlongitude\": -79.0,\n",
    "    \"maxlongitude\": -69.9,\n",
    "    \"jsonerror\": \"true\"\n",
    "}\n",
    "\n",
    "chile_response = requests.get(chile_url, params=chile_params)\n",
    "\n",
    "if chile_response.status_code == 200:\n",
    "    # Si la solicitud fue exitosa, guardamos la respuesta en un archivo CSV\n",
    "    with open(\"chile_earthquake_data.csv\", \"w\") as chile_file:\n",
    "        chile_file.write(chile_response.text)\n",
    "    print(\"Archivo CSV de Chile descargado exitosamente.\")\n",
    "else:\n",
    "    print(\"Error al realizar la solicitud de Chile:\", chile_response.status_code)\n",
    "\n",
    "japan_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "japan_params = {\n",
    "    \"format\": \"csv\",\n",
    "    \"starttime\": \"2012-01-01 00:00:00\",\n",
    "    \"endtime\": \"2023-06-26 23:59:59\",\n",
    "    \"minlatitude\": 27.0,\n",
    "    \"maxlatitude\": 44.0,\n",
    "    \"minlongitude\": 132.78,\n",
    "    \"maxlongitude\": 145.53,\n",
    "    \"minmagnitude\": 3,\n",
    "    \"orderby\": \"time-asc\"\n",
    "}\n",
    "\n",
    "japan_response = requests.get(japan_url, params=japan_params)\n",
    "\n",
    "if japan_response.status_code == 200:\n",
    "    # Si la solicitud fue exitosa, guardamos la respuesta en un archivo CSV\n",
    "    with open(\"japan_earthquake_data.csv\", \"w\", encoding=\"utf-8\") as japan_file:\n",
    "        japan_file.write(japan_response.text)\n",
    "    print(\"Archivo CSV de Japón descargado exitosamente.\")\n",
    "else:\n",
    "    print(\"Error al realizar la solicitud de Japón:\", japan_response.status_code)\n",
    "\n",
    "usa_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "usa_params = {\n",
    "    \"format\": \"csv\",\n",
    "    \"starttime\": \"2012-01-01 00:00:00\",\n",
    "    \"endtime\": \"2023-06-26 23:59:59\",\n",
    "    \"maxlatitude\": 50,\n",
    "    \"minlatitude\": 24.6,\n",
    "    \"maxlongitude\": -65,\n",
    "    \"minlongitude\": -125,\n",
    "    \"minmagnitude\": 3,\n",
    "    \"orderby\": \"time-asc\"\n",
    "}\n",
    "\n",
    "usa_response = requests.get(usa_url, params=usa_params)\n",
    "\n",
    "if usa_response.status_code == 200:\n",
    "    # Si la solicitud fue exitosa, guardamos la respuesta en un archivo CSV\n",
    "    with open(\"usa_earthquake_data.csv\", \"w\") as usa_file:\n",
    "        usa_file.write(usa_response.text)\n",
    "    print(\"Archivo CSV de Estados Unidos descargado exitosamente.\")\n",
    "else:\n",
    "    print(\"Error al realizar la solicitud de Estados Unidos:\", usa_response.status_code)\n",
    "\n",
    "# Leer los archivos CSV y asignar el nombre del país a cada DataFrame\n",
    "df_chile = pd.read_csv('chile_earthquake_data.csv', encoding='latin-1')\n",
    "df_chile['Country'] = 'Chile'\n",
    "\n",
    "df_japan = pd.read_csv('japan_earthquake_data.csv', encoding='utf-8')\n",
    "df_japan['Country'] = 'Japan'\n",
    "\n",
    "df_usa = pd.read_csv('usa_earthquake_data.csv', encoding='latin-1')\n",
    "df_usa['Country'] = 'USA'\n",
    "\n",
    "# Concatenar los DataFrames uno debajo del otro\n",
    "df = pd.concat([df_chile, df_japan, df_usa], ignore_index=True)\n",
    "\n",
    "# Definir el diccionario de mapeo de nombres de columnas\n",
    "nombres_columnas = {\n",
    "    'time': 'dt_sismo',\n",
    "    'depth':'profundidad',\n",
    "    'magType': 'tipo_magnitud',\n",
    "    'updated': 'dt_actualizacion',\n",
    "    'place': 'ubicacion',\n",
    "    'type': 'tipo',\n",
    "    'horizontalError': 'error_horizontal',\n",
    "    'depthError': 'error_profundidad',\n",
    "    'magError': 'error_magnitud',\n",
    "    'status': 'estado',\n",
    "    'locationSource': 'fuente_localizacion',\n",
    "    'magSource': 'fuente_mag',\n",
    "    'Country': 'pais'\n",
    "}\n",
    "\n",
    "# Renombrar las columnas del DataFrame\n",
    "df = df.rename(columns=nombres_columnas)\n",
    "\n",
    "# Convertir la columna 'dt_sismo' en un objeto datetime\n",
    "df['dt_sismo'] = pd.to_datetime(df['dt_sismo'])\n",
    "# Extraer la fecha de la columna 'dt_sismo'\n",
    "df['fecha_sismo'] = df['dt_sismo'].dt.strftime('%Y-%m-%d')\n",
    "# Extraer hora de la columna 'dt_sismo'\n",
    "df['hora_sismo'] = pd.to_datetime(df['dt_sismo'], format='%H:%M:%S').dt.time\n",
    "# Obtener la lista de columnas del DataFrame\n",
    "columnas = df.columns.tolist()\n",
    "\n",
    "# Mover las columnas 'fecha_sismo' y 'hora_sismo'\n",
    "columnas.remove('fecha_sismo')\n",
    "columnas.remove('hora_sismo')\n",
    "columnas.insert(1, 'fecha_sismo')\n",
    "columnas.insert(2, 'hora_sismo')\n",
    "\n",
    "# Reindexar el DataFrame con las columnas en el nuevo orden\n",
    "df = df.reindex(columns=columnas)\n",
    "\n",
    "# Convertir la columna 'dt_actualizacion' en un objeto datetime\n",
    "df['dt_actualizacion'] = pd.to_datetime(df['dt_actualizacion'])\n",
    "# Extraer la fecha de la columna 'dt_actualizacion'\n",
    "df['fecha_actualizacion'] = df['dt_actualizacion'].dt.strftime('%Y-%m-%d')\n",
    "# Extraer hora de la columna 'dt_actualizacion'\n",
    "df['hora_actualizacion'] = pd.to_datetime(df['dt_actualizacion'], format='%H:%M:%S').dt.time\n",
    "# Obtener la lista de columnas del DataFrame\n",
    "columnas = df.columns.tolist()\n",
    "\n",
    "# Mover las columnas 'fecha_sismo' y 'hora_sismo'\n",
    "columnas.remove('fecha_actualizacion')\n",
    "columnas.remove('hora_actualizacion')\n",
    "columnas.insert(15, 'fecha_actualizacion')\n",
    "columnas.insert(16, 'hora_actualizacion')\n",
    "\n",
    "# Reindexar el DataFrame con las columnas en el nuevo orden\n",
    "df = df.reindex(columns=columnas)\n",
    "\n",
    "df.to_csv('df.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
